{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6cc42de-d812-4867-83a0-9b053ff2dd1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U chromadb==0.3.22 langchain==0.0.164 transformers==4.29.0 accelerate==0.19.0 requests==2.28.2 pysqlite3-binary bs4 nvidia-cudnn-cu11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e4d38f-f179-4b2f-b863-9e38ef80ac6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install openai==0.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfb5857-7f44-4f98-a175-52f71369da17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Iterator\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pyspark.sql.functions as F\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain import PromptTemplate\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import AzureOpenAI\n",
    "import openai\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import LLMChain\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b75eecc-5a7b-4b89-a492-08eb2fc29688",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path)\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "model_name = os.getenv('MODEL_NAME')\n",
    "openai.api_type = os.getenv('OPENAI_API_TYPE')\n",
    "openai.api_base = os.getenv('OPENAI_API_BASE')\n",
    "openai.api_version = os.getenv('OPENAI_API_VERSION')\n",
    "serp_api = os.getenv('SERPAPI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5a1247e-5354-44fb-9551-9ca24c03bdb1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = sqlContext.sql(\"SELECT * FROM table\").toPandas()\n",
    "\n",
    "# Concatenate values in each row and create a new column 'concatenated'\n",
    "data['text'] = data.apply(lambda row: ', '.join(f'{col} is {value}' for col, value in zip(data.columns, row)), axis=1)\n",
    "a = pd.DataFrame(data['text'])\n",
    "\n",
    "spark_df = spark.createDataFrame(a)\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "656dac57-40bd-432c-abf7-4ddc4885eddd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "docs_df = sqlContext.sql(\"SELECT * FROM table\")\n",
    "@F.pandas_udf(\"string\")\n",
    "def summarize(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    # Load the model for summarization\n",
    "    torch.cuda.empty_cache()\n",
    "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device_map=\"auto\")\n",
    "    def summarize_txt(text):\n",
    "      if len(text) > 5000:\n",
    "        return summarizer(text)[0]['summary_text']\n",
    "      return text\n",
    " \n",
    "    for serie in iterator:\n",
    "        # get a summary for each row\n",
    "        yield serie.apply(summarize_txt)\n",
    " \n",
    "# We won't run it as this can take some time in the entire dataset. In this demo we set repartition to 1 as we just have 1 GPU by default.\n",
    "docs_df = docs_df.repartition(1).withColumn(\"text_short\", summarize(\"text\"))\n",
    "docs_df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"flight_training_dataset\")\n",
    "display(spark.table(\"flight_training_dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eac889b-3b88-47e6-9f91-40686d9bfa66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare a directory to store the document database. Any path on `/dbfs` will do.\n",
    "dbutils.widgets.dropdown(\"reset_vector_database\", \"false\", [\"false\", \"true\"], \"Recompute embeddings for chromadb\")\n",
    "flight_vector_db_path = \"/dbdemos/product/llm/vector_db_flight_test\"\n",
    " \n",
    "# Don't recompute the embeddings if the're already available\n",
    "compute_embeddings = dbutils.widgets.get(\"reset_vector_database\") == \"true\" #or is_folder_empty(flight_vector_db_path)\n",
    " \n",
    "if compute_embeddings:\n",
    "  print(f\"creating folder {flight_vector_db_path} under our blob storage (dbfs)\")\n",
    "  dbutils.fs.rm(flight_vector_db_path, True)\n",
    "  dbutils.fs.mkdirs(flight_vector_db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e546ca-3d4d-4e33-8748-071f71b820b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flight_vector_db_path = \"/dbfs/dbdemos/product/llm/vector_db_flight_test\"\n",
    "hf_embed = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "chroma_db = Chroma(collection_name=\"flight_docs\", embedding_function=hf_embed, persist_directory=flight_vector_db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60763bcc-94f3-4887-91c4-3901a3b15df2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_qa_chain():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Defining our prompt content.\n",
    "    # langchain will load our similar documents as {context}\n",
    "    # Update the template to handle two human inputs\n",
    "    template =  \"\"\"\n",
    "    Do not generate user responses on your own and do not repeating same questions.\n",
    "\n",
    "    You are a helpful flight ticket assistant. Your only task is to help user to understand their flight ticket purchased information. \n",
    "    The flight ticket purchased information are: departure date, flight number, ancillary purchases and flight-destination.\n",
    "    To get the flight ticket purchased information, you need to collect information in the conversation such as PNR and email address. \n",
    "    Collect all of the information one by one. \n",
    "    After collecting all the information, make sure you display the details to the user at the end in this format:\n",
    "    PNR: \n",
    "    Email Address:\n",
    "\n",
    "    After user confirm the information is correct.\n",
    "    Ask user what flight ticket purchased information that user want to know. \n",
    "    {context}\n",
    "    Based on the context provide relevant information to user.\n",
    "    For the flight ticket purchased information that do not covered. \n",
    "    Respond with just 'sorry, we dont have the information'.\n",
    "     \n",
    "    {chat_history}\n",
    " \n",
    "    {human_input}\n",
    "    \"\"\"\n",
    "\n",
    "    # Increase max_new_tokens for a longer response\n",
    "    # Other settings might give better results! Play around\n",
    "    prompt = PromptTemplate(input_variables=['context', 'human_input', 'chat_history'], template=template)\n",
    "    # model_name = \"databricks/dolly-v2-3b\" # can use dolly-v2-3b, dolly-v2-7b or dolly-v2-12b for smaller model and faster inferences.\n",
    "\n",
    "    # instruct_pipeline = pipeline(model=model_name, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", \n",
    "    #                            return_full_text=True, max_new_tokens=256, top_p=0.95, top_k=50)\n",
    "\n",
    "    # hf_pipe = HuggingFacePipeline(pipeline=instruct_pipeline)\n",
    "    llm = AzureOpenAI(model_name=model_name,engine=model_name)\n",
    "    # Models we'll use to summarize our chat history\n",
    "    # We could use one of these models: https://huggingface.co/models?filter=summarization. facebook/bart-large-cnn gives great results, we'll use t5-small for memory\n",
    "    summarize_model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "    summarize_tokenizer = AutoTokenizer.from_pretrained(\"t5-small\", padding_side=\"left\", model_max_length = 512)\n",
    "    pipe_summary = pipeline(\"summarization\", model=summarize_model, tokenizer=summarize_tokenizer)\n",
    "    hf_summary = HuggingFacePipeline(pipeline=pipe_summary)\n",
    "    memory = ConversationSummaryBufferMemory(llm=hf_summary, memory_key=\"chat_history\", input_key=\"human_input\", max_token_limit=500, human_prefix = \"\", ai_prefix = \"\")\n",
    "\n",
    "    # Set verbose=True to see the full prompt:\n",
    "    print(\"loading chain, this can take some time...\")\n",
    "    return load_qa_chain(llm=llm, chain_type=\"stuff\", prompt=prompt, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63eb4d5-e6a4-41f5-bcf0-0d20ff95d8f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/langchain/llms/openai.py:696: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "loading chain, this can take some time...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>Hi! I'm a chat bot specialized in flight info. How Can I help you today?</h1>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<h1>Hi! I'm a chat bot specialized in flight info. How Can I help you today?</h1>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ChatBot():\n",
    "  def __init__(self, db):\n",
    "    self.reset_context()\n",
    "    self.db = db\n",
    " \n",
    "  def reset_context(self):\n",
    "    self.sources = []\n",
    "    self.discussion = []\n",
    "    # Building the chain will load Dolly and can take some time depending on the model size and your GPU\n",
    "    self.qa_chain = build_qa_chain()\n",
    "    displayHTML(\"<h1>Hi! I'm a chat bot specialized in flight info. How Can I help you today?</h1>\")\n",
    " \n",
    "  def get_similar_docs(self, question, similar_doc_count):\n",
    "    return self.db.similarity_search(question, k=similar_doc_count)\n",
    " \n",
    "  def chat(self, question):\n",
    "    # Keep the last 3 discussion to search similar content\n",
    "    self.discussion.append(question)\n",
    "    similar_docs = self.get_similar_docs(\" \\n\".join(self.discussion[-3:]), similar_doc_count=1)\n",
    " \n",
    "    result = self.qa_chain({\"input_documents\": similar_docs, \"human_input\": question})\n",
    "\n",
    "    # Cleanup the answer for better display:\n",
    "    answer = result['output_text'].capitalize()\n",
    "    result_html = f\"<p><blockquote style=\\\"font-size:24\\\">{question}</blockquote></p>\"\n",
    "    result_html += f\"<p><blockquote style=\\\"font-size:18px\\\">{answer}</blockquote></p>\"\n",
    "    result_html += \"<p><hr/></p>\"\n",
    "    displayHTML(result_html)\n",
    " \n",
    "chat_bot = ChatBot(chroma_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b001169d-9c9f-4157-a8c3-eab94e58f6f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat_bot.chat(\"Hi\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Vector Search",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
